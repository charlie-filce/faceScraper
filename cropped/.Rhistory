list("min" = ~ min(steam$median_playtime),
"max" = ~ max(steam$median_playtime),
"mean (sd)" = ~ qwraps2::mean_sd(steam$median_playtime)),
"Price" =
list("min" = ~ min(steam$price),
"max" = ~ max(steam$price),
"mean (sd)" = ~ qwraps2::mean_sd(steam$price))
)
summary_table(steam,our_summary1)
library(dplyr)
steam %>%
dplyr::select(steam$required_age, steam$achievements, steam$positive_ratings, steam$negative_ratings, steam$average_playtime, steam$median_playtime) %>%
summary_table(.)
steam %>%
dplyr::select(steam$required_age, steam$achievements,steam$positive_ratings, steam$negative_ratings, steam$average_playtime, steam$median_playtime) %>%
summary_table(.)
steam %>%
dplyr::select(steam$required_age, steam$achievements,steam$positive_ratings, steam$negative_ratings, steam$average_playtime, steam$median_playtime) %>%
qsummary(.)
library(knitr)
install.packages("summarytools")
library(summarytools)
steam_summary = dfSummary(steam)
steam_summary_df = as.data.frame(steam$required_age, steam$achievements, steam$positive_ratings, steam$negative_ratings, steam$average_playtime, steam$median_playtime, steam$price)
steam_summary = dfSummary(steam_summary_df)
kable(steam_summary_df)
View(steam_summary)
View(steam_summary_df)
steam_summary_df = steam[steam$required_age, steam$achievements, steam$positive_ratings, steam$negative_ratings, steam$average_playtime, steam$median_playtime, steam$price]
steam_summary_df = steam[c(steam$required_age, steam$achievements, steam$positive_ratings, steam$negative_ratings, steam$average_playtime, steam$median_playtime, steam$price)]
colnames(steam)
steam_summary_df = steam[c(steam$required_age, steam$achievements, steam$positive_ratings, steam$negative_ratings, steam$average_playtime, steam$median_playtime, steam$price)]
steam_summary_df = steam[c(required_age, achievements, positive_ratings, negative_ratings, average_playtime, median_playtime, price)]
steam_summary_df = steam[c("required_age", "achievements", "positive_ratings", "negative_ratings", "average_playtime", "median_playtime", "price")]
View(steam_summary_df)
steam_summary = dfSummary(steam_summary_df)
kable(steam_summary_df)
library(ggplot2)
library(factoextra)
#------------------------------------------------
### Auto data
#------------------------------------------------
# using Auto data
library(ISLR)
data(Auto)
library(cowplot)
# use ggpairs to see if the clusters are meaningful
library(GGally)
library(devtools)
library(ggbiplot)
# do the above with a sampled data (20 observations)
# sampled data
library(ISLR)
data(Auto)
Auto_sample = Auto[sample(1:nrow(Auto), size = 20),]
Auto_sampled = Auto[sample(1:nrow(Auto), size = 20),]
# scale the data
Auto_sampled[,1:7] = scale(Auto_sampled[,1:7])
# run PCA
pca_auto = prcomp(Auto_sampled[,1:7], center = TRUE, scale. = TRUE)
summary(pca_auto)
# visualize with car names on it
ggbiplot(pca_auto)
ggbiplot(pca_auto, labels = Auto_sampled$name)
# more options
Auto_sampled$country = ifelse(Auto_sampled$origin == 1, "American",
ifelse(Auto_sampled == 2, "European", "Japanese"))
ggbiplot(pca_auto, labels = Auto_sampled$name, groups = Auto_sampled$country)
ggbiplot(pca_auto, labels = Auto_sampled$name, groups = Auto_sampled$country, ellipse = TRUE)
# installing required packages
install.packages("xgboost")  # for boosting
install.packages("FeatureHashing")  # for feature generation
install.packages("tm")  # for text mining
install.packages("SnowballC") # for text stemming
install.packages("wordcloud") # word-cloud generator
install.packages("RColorBrewer") # color palettes
install.packages("RColorBrewer")
install.packages("RSentiment")  # for pre-trained sentiment analyses
library(ggplot2)
library(plotROC)
library(tm)
library(SnowballC)
library(RColorBrewer)
library(wordcloud)
library(xgboost)
library(FeatureHashing)
library(Matrix)
library(RSentiment)
# imdb review file is store as a tsv file.
# tsv file is very similar to csv except it uses tabs (\t)
# instead of comma as delimeter
imdb <- read.delim("C:\\Users\\charl\\OneDrive\\Desktop\\MGSC_310\\labeledTrainData.tsv",quote="", as.is=T)
dim(imdb)
# explore the data
View(imdb)
install.packages("plotROC")
# sample review
imdb$review[457]
strwrap(imdb$review[457], width = 80)
# ratio of positives and negatives
table(imdb$sentiment)
# a function to
#     1. remove the punctuation,
#     2. lowercase, and
#     3. create a (hashed) features.
cleantext <- function(df){
df$review <- tolower(gsub("[^[:alnum:] ]"," ", df$review))
features <- hashed.model.matrix(~ split(review, delim = " ", type = "tf-idf"),
data = imdb, hash.size = 2^16, signed.hash = FALSE)
}
data = cleantext(imdb)
dim(data)   # it resulted into more than 65000 features!
view(data)
View(data)
as.integer(which(data[457, ] != 0))
# creating a corpus
docs <- Corpus(VectorSource(imdb$review[1:1000]))
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
# Remove some punctuations
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
docs <- tm_map(docs, toSpace, "><br")
docs <- tm_map(docs, toSpace, ".<br")
docs <- tm_map(docs, toSpace, "<br")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
#docs <- tm_map(docs, removeWords, c("movie", "film"))
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# creating the term-document matrix:
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
# top 10 words in the corpus
head(d, 10)
set.seed(2019)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
# find the words that appeared at least 100 times
findFreqTerms(dtm, lowfreq = 100)
# find the words that are mostly correlated with "thriller"
findAssocs(dtm, terms = "thriller", corlimit = 0.3)
# create a bar plot for top 10 frequent words
barplot(d[1:10,]$freq, las = 2, names.arg = d[1:10,]$word,
col ="lightblue", main ="Most frequent words",
ylab = "Word frequencies")
set.seed(2019)
train <- sample(1:nrow(imdb), size=20000)
# we create xgb.DMatrix for xgboost
dtrain <- xgb.DMatrix(data[train,], label = imdb$sentiment[train])
dvalid <- xgb.DMatrix(data[-train,], label = imdb$sentiment[-train])
# boosted logistic (generalized linear)
boosted_linear <- xgboost(booster = "gblinear", data =dtrain,
eta = 0.02, eval_metric = "error",
nrounds = 10, objective = "binary:logistic",
verbose = 1)
# boosted tree
boosted_tree <- xgboost(booster = "gbtree", data =dtrain,
max_depth = 10, eta = 0.05, nthread = 4,
colsample_bytree = 0.5, eval_metric = "error",
nrounds = 400, objective = "binary:logistic",
verbose = 1)
preds_DF <- data.frame(scores_lm = predict(boosted_linear, newdata=dvalid),
scores_tree = predict(boosted_tree, newdata=dvalid),
test_labels = imdb$sentiment[-train])
# boosted linear
ggplot(preds_DF,
aes(m = scores_lm,
d = test_labels)) +
geom_roc(cutoffs.at = c(.99,.9,.8,.7,.6,.5,.4,.3,.2,.1,0))
library(plotROC)
# boosted linear
ggplot(preds_DF,
aes(m = scores_lm,
d = test_labels)) +
geom_roc(cutoffs.at = c(.99,.9,.8,.7,.6,.5,.4,.3,.2,.1,0))
# boosted tree
ggplot(preds_DF,
aes(m = scores_tree,
d = test_labels)) +
geom_roc(cutoffs.at = c(.99,.9,.8,.7,.6,.5,.4,.3,.2,.1,0))
# accuracy
predicted_sentiment <- ifelse(preds_DF$scores_tree>0.5,"Pos","Neg")
table(predicted_sentiment,preds_DF$test_labels)
bad_review <- "This is a very bad movie. You shouldn't watch it. Waste of time"
good_review <- "I recommend this movie. The acting is excellent. It is a must watch. Directing is very good"
test_df <- data.frame(review = c(bad_review,good_review))
# process the test
test_processed <- tolower(gsub("[^[:alnum:] ]"," ", test_df$review))
test_processed <- hashed.model.matrix(~ split(review, delim = " ", type = "tf-idf"),
data = test_df, hash.size = 2^16, signed.hash = FALSE)
# predict
predict(boosted_linear, newdata=test_processed)
predict(boosted_tree, newdata=test_processed)
sentences <- c("This is a good text",
"This is a bad text",
"This is a really bad text",
"This is horrible")
calculate_total_presence_sentiment(sentences)
calculate_sentiment(sentences)
calculate_score(sentences)
# Let's see if the model can detect sarcasm!
calculate_total_presence_sentiment("Really, Sherlock? No! You are clever.")
calculate_total_presence_sentiment("Nice perfume. How long did you marinate in it?")
install.packages("installr")
library(installr)
updateR()
#########################################################
#This allows you to load a contingency table manually in R
tuberculosis <- matrix(c(63,66,36,289), nrow = 2, byrow = TRUE)
colnames(tuberculosis) <- c("High", "Low") #replaces the column header with words instead of numbers
rownames(tuberculosis) <- c("Positive", "Negative") #replaces the row header with words instead of numbers
tuberculosis
############################################################
prop.tuberculosis <- prop.table(tuberculosis, 2)
barplot(prop.tuberculosis,
beside = TRUE,
col = c("lightblue","seagreen"),
ylim=c(0,1),
main="Barplot of Tuberculosis")
legend("topleft", rownames(prop.tuberculosis),
bty="n",
col = c("lightblue", "seagreen"),
pch=rep(15, length(rownames(prop.tuberculosis))))
legend("topleft", rownames(prop.tuberculosis),
bty="y",
col = c("lightblue", "seagreen"),
pch=rep(15, length(rownames(prop.tuberculosis))))
legend("topleft", rownames(prop.tuberculosis),
bty="n",
col = c("lightblue", "seagreen"),
pch=rep(15, length(rownames(prop.tuberculosis))))
legend("topleft", rownames(prop.tuberculosis),
bty="n",
col = c("lightblue", "seagreen"),
pch=rep(15, length(rownames(prop.tuberculosis))))
barplot(prop.tuberculosis,
beside = TRUE,
col = c("lightblue","seagreen"),
ylim=c(0,1),
main="Barplot of Tuberculosis")
legend("topleft", rownames(prop.tuberculosis),
bty="n",
col = c("lightblue", "seagreen"),
pch=rep(15, length(rownames(prop.tuberculosis))))
?histogram
temp = sample(1:2712,35,replace=FALSE)
histogram(temp)
histogram(iris)
view(iris)
View(iris)
histogram(iris$Sepal.Length,iris)
example.data = c(8,5,4,12,15,5,7)
boxplot(example.data)
boxplot(example.data,horizontal = TRUE)
fivenum(Orange)
fivenum(Orange$circumference)
summary(Orange$circumference)
hist(iris$Sepal.Length,iris)
?hist()
hist(Orange$age,breaks = 10)
?hist()
hist(Orange$age,breaks = 10,freq = FALSE)
boxplot(Orange$age)
boxplot(example.data)
boxplot(Orange$age)
?boxplot.stats
?hist()
summary(Orange$circumference)
37/176
5/485
5/50
5/555
480/505
8/107
891/893
457/(457+3199)
457*3199/260*2860
(457*3199)/(260*2860)
radiation = c(1.18,1.41,1.49,1.04,1.45,.74,.89,1.42,1.45,.51,1.38)
mean(radiation)
median(radiation)
sqrt(var(radiation))/mean(radiation)
7.1-5.4
5.4-(1.5*1.7)
5.4-1.5*1.7
counts = c(263,206,185,246,188,191,308,262,198,235,646)
sqrt(var(counts))
counts = c(263,206,185,246,188,191,308,262,198,253,646)
sqrt(var(counts))
boxplot.stats(counts)
?boxplot.stats
11.3^2
.408/.518
require(datasets)
data("HairEyeColor")
Hair.Color = margin.table(HairEyeColor,1)
chisq.test(Hair.Color)
heads <- rbinom(1, size = 100, prob = .5)
prop.test(heads, 100)          # continuity correction TRUE by default
prop.test(heads, 100, correct = FALSE)
prop.test(heads, 100)          # continuity correction TRUE by default
install.packages("BSDA")
prop.test(25, 110, p = .3, alternative = "two.sided",
correct = TRUE)
prop.test(c(93,619), c(180,1716), p = .05, alternative = "two.sided",
correct = TRUE)
prop.test(c(93,619), c(180,1716), p = NULL, alternative = "two.sided",
correct = TRUE)
prop.test(c(63,36), c(129,325), p = NULL, alternative = "two.sided",
correct = TRUE)
prop.test(c(63,36), c(129,325), p = NULL, alternative = "greater",
correct = TRUE)
prop.test(c(7,36,42,25),c(27,164,190,141),conf.level = 1-.05)
tbl = table(c(1,1,3,31,13),c(2,13,28,23,5))
tbl
temp = matrix(c(1,1,3,31,13,2,13,28,23,5),ncol = 5)
temp
temp = matrix(c(1,2,1,13,3,28,31,23,13,5),ncol = 5)
temp
colnames(temp) = c("Very frequently","frequently","occasionally","rarely","never")
temp
rownames(temp) = c("Attending","Resident")
temp
temp = as.table(temp)
temp
chisq.test(temp)
t.test(c(350,200,240,290,90,370,240),
c(480,130,250,310,280,1450,280),
paired = TRUE, alternative = "two.sided")
t.test(c(480,130,250,310,280,1450,280),
c(350,200,240,290,90,370,240),
paired = TRUE, alternative = "two.sided")
flakes <- c(4.61,6.42,5.40,4.54,3.98,3.82,5.01,4.34,3.80,4.56,5.35,3.89,2.25,4.24)
bran <- c(3.84,5.57,5.85,4.80,3.68,2.96,4.41,3.72,3.49,3.84,5.26,3.73,1.84,4.14)
change <- flakes-bran
t.test(change, alternative = "greater", conf.level = 0.95)
t.test(c(53,56,60,60,78,87,102,117,134,160,277),
c(46,52,58,59,77,78,80,81,84,103,114,115,133,134,175,175),
paired = TRUE, alternative = "two.sided")
t.test(c(53,56,60,60,78,87,102,117,134,160,277),
c(46,52,58,59,77,78,80,81,84,103,114,115,133,134,175,175),
paired = FALSE, alternative = "two.sided")
prop.test(c(24,28), c(97,161), p = NULL, alternative = "two.sided",
correct = TRUE)
prop.test(c(24,28), c(97,161), p = .01, alternative = "two.sided",
correct = TRUE)
prop.test(c(24,28), c(97,161), p = NULL, alternative = "two.sided",
correct = TRUE, conf.level = 0.99)
prop.test(c(50,45,5), rep(100,3), p = c(.3,.6,.1), alternative = "two.sided",
correct = TRUE)
require(BSDA)
tsum.test(mean.x = 16.51, s.x = 9.59, n.x = 77,
mu=14, alternative = "two.sided",
conf.level = .9)
prop.test(c(60.6,12.0,56.0,75.2,12.5,29.7,57.2,62.7,28.7,66.0,25.2,40.1),
c(47.5,13.3,33.0,55.2,21.9,27.9,54.3,13.9,8.9,46.1,29.8,36.2),
p = NULL, alternative = "two.sided",
correct = TRUE)
t.test(c(60.6,12.0,56.0,75.2,12.5,29.7,57.2,62.7,28.7,66.0,25.2,40.1),
c(47.5,13.3,33.0,55.2,21.9,27.9,54.3,13.9,8.9,46.1,29.8,36.2),
paired = TRUE, alternative = "two.sided")
t.test(c(47.5,13.3,33.0,55.2,21.9,27.9,54.3,13.9,8.9,46.1,29.8,36.2),
c(60.6,12.0,56.0,75.2,12.5,29.7,57.2,62.7,28.7,66.0,25.2,40.1),
paired = TRUE, alternative = "two.sided")
t.test(c(60.6,12.0,56.0,75.2,12.5,29.7,57.2,62.7,28.7,66.0,25.2,40.1),
c(47.5,13.3,33.0,55.2,21.9,27.9,54.3,13.9,8.9,46.1,29.8,36.2),
paired = TRUE, alternative = "two.sided")
.7885 * (9.1921/4.5984)
b = .7885 * (9.1921/4.5984)
36.0667-(b*7.9133)
t.test(c(73,58,67,93,33,18,147),
c(24,27,49,59,0,11,43),
paired = TRUE, alternative = "two.sided")
temp = matrix(c(17,130,218,428),ncol = 2)
temp
colnames(temp) = c("Yes","No")
rownames(temp) = c("Yes","No")
temp = as.table(temp)
temp
chisq.test(temp)
prop.test(c(25,56,19), rep(100,3), p = c(.3,.6,.1), alternative = "two.sided",
correct = TRUE)
0.7885^2
chisq.test(temp)
?chisq.test
36.0667-(b*7.9133)
cor(c(73,58,67,93,33,18,147),
c(24,27,49,59,0,11,43))
t.test(c(73,58,67,93,33,18,147),
c(24,27,49,59,0,11,43),
paired = TRUE, alternative = "two.sided")
#manual image entry
images = c("luke_S.jpg","bright.jpg","frodo_B.jpg","obama.jpg")
#cropping process
setwd("C:\\Users\\charl\\OneDrive\\Desktop\\faceScraper\\cropped")
for (i in 1:length(images)) {
imageName = images[i]
print(imageName)
currentImage = image_read(imageName)
faces = image_detect_faces(currentImage)
x = faces$detections$x
y = faces$detections$y
width = faces$detections$width
height = faces$detections$height
img = image_crop(currentImage, geometry_area(x=(x/1.5),y=(y/2),width = (3*width),height = (3*height)))
img = image_scale(img,"300x300")
image_write(img, path = imageName, format = "jpg")
}
setwd("C://Users//charl//OneDrive//Desktop//lifeExpectancy")
lifeExpectancy = read.csv("led.csv",header=TRUE)
setwd("C:\\Users\\charl\\OneDrive\\Desktop\\faceScraper") #replace "cropped_directory" with the path for where you saved the "cropped" folder
image_df = read.csv("img_test.csv",header=TRUE) #replace "csv_name" with the name of your csv
images = image_df[["image_name"]] #replace "image_column_name" with the name of the column your images are in
print(images)
images = image_df[["image_name"]] #replace "image_column_name" with the name of the column your images are in
print(images)
setwd("C:\\Users\\charl\\OneDrive\\Desktop\\faceScraper") #replace "cropped_directory" with the path for where you saved the "cropped" folder
image_df = read.csv("img_test.csv",header=TRUE) #replace "csv_name" with the name of your csv
images = image_df[["image_name"]] #replace "image_column_name" with the name of the column your images are in
print(images)
print(images[1])
images = image_df[["image_name"]] #replace "image_column_name" with the name of the column your images are in
print(images[1])
print(images[2])
print(image_df)
images = image_df[["image.name"]] #replace "image_column_name" with the name of the column your images are in
print(image_df)
print(images)
setwd("C:\\Users\\charl\\OneDrive\\Desktop\\faceScraper\\cropped") #replace "cropped_directory" with the path for where you saved the "cropped" folder
for (i in 1:length(images)) {
imageName = images[i]
print(imageName)
currentImage = image_read(imageName)
faces = image_detect_faces(currentImage)
x = faces$detections$x
y = faces$detections$y
width = faces$detections$width
height = faces$detections$height
img = image_crop(currentImage, geometry_area(x=(x/1.5),y=(y/2),width = (3*width),height = (3*height)))
img = image_scale(img,"300x300")
image_write(img, path = imageName, format = "jpg")
}
image_df = as.data.frame(read.csv("img_test.csv",header=TRUE)) #replace "csv_name" with the name of your csv
setwd("C:\\Users\\charl\\OneDrive\\Desktop\\faceScraper") #replace "cropped_directory" with the path for where you saved the "cropped" folder
image_df = as.data.frame(read.csv("img_test.csv",header=TRUE)) #replace "csv_name" with the name of your csv
images = image_df[["image.name"]] #replace "image_column_name" with the name of the column your images are in
print(images)
print(images[1])
images = c(image_df[["image.name"]]) #replace "image_column_name" with the name of the column your images are in
print(images)
setwd("C:\\Users\\charl\\OneDrive\\Desktop\\faceScraper") #replace "cropped_directory" with the path for where you saved the "cropped" folder
image_df = as.data.frame(read.csv("img_test.csv",header=TRUE)) #replace "csv_name" with the name of your csv
images = image_df[["image.name"]] #replace "image_column_name" with the name of the column your images are in
images = as.array(images)
print(images)
images = as.character(images)
print(images)
setwd("C:\\Users\\charl\\OneDrive\\Desktop\\faceScraper\\cropped") #replace "cropped_directory" with the path for where you saved the "cropped" folder
for (i in 1:length(images)) {
imageName = images[i]
print(imageName)
currentImage = image_read(imageName)
faces = image_detect_faces(currentImage)
x = faces$detections$x
y = faces$detections$y
width = faces$detections$width
height = faces$detections$height
img = image_crop(currentImage, geometry_area(x=(x/1.5),y=(y/2),width = (3*width),height = (3*height)))
img = image_scale(img,"300x300")
image_write(img, path = imageName, format = "jpg")
}
library(magick)
library(image.libfacedetection)
setwd("C:\\Users\\charl\\OneDrive\\Desktop\\faceScraper\\cropped") #replace "cropped_directory" with the path for where you saved the "cropped" folder
for (i in 1:length(images)) {
imageName = images[i]
print(imageName)
currentImage = image_read(imageName)
faces = image_detect_faces(currentImage)
x = faces$detections$x
y = faces$detections$y
width = faces$detections$width
height = faces$detections$height
img = image_crop(currentImage, geometry_area(x=(x/1.5),y=(y/2),width = (3*width),height = (3*height)))
img = image_scale(img,"300x300")
image_write(img, path = imageName, format = "jpg")
}
images = as.character(image_df[["image.name"]]) #replace "image_column_name" with the name of the column your images are in
print(images)
for (i in 1:length(images)) {
imageName = images[i]
print(imageName)
currentImage = image_read(imageName)
faces = image_detect_faces(currentImage)
x = faces$detections$x
y = faces$detections$y
width = faces$detections$width
height = faces$detections$height
img = image_crop(currentImage, geometry_area(x=(x/1.5),y=(y/2),width = (3*width),height = (3*height)))
img = image_scale(img,"300x300")
image_write(img, path = imageName, format = "jpg")
}
